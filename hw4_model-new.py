# -*- coding: utf-8 -*-
"""Copy of hw4.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1US0dcVMoc0egQox5Z4yaaE08k0gE0c1R
"""

# Commented out IPython magic to ensure Python compatibility.
# from google.colab import drive
# drive.mount('/content/drive/',force_remount=True)
# # %cd drive/MyDrive/11747
# !pip install git+https://github.com/huggingface/transformers
# !pip install  datasets pip install sentencepiece
import json
import torch
import torch.nn as nn
import numpy as np
# from datasets import load_dataset, load_metric
from torch.utils.data import Dataset, DataLoader
from transformers.trainer import default_data_collator
from transformers import AlbertTokenizer
# from transformers import AlbertForSequenceClassification
from transformers import AlbertForPreTraining
from transformers import AlbertConfig, AlbertModel
from transformers.models.albert.modeling_albert import AlbertPreTrainedModel

from lf_tokenizer import LFTokenizer
from lflstm import LFModel

import argparse

"""
class BertDataset(Dataset):
    # def __init__(self, dataset, tokenizer, lf_tokenizer, batchsize):
    def __init__(self, dataset, tokenizer, batchsize):
        self.dataset = dataset
        self.batch_size = batchsize
        self.tokenizer = tokenizer
        # self.lf_tokenizer = lf_tokenizer
    def __len__(self):
        return len(self.dataset)

    def __getitem__(self, idx):
        ele = self.dataset[idx]
        return ele

    def pad_data(self, data):
        sents1 = [x['sentence1'] for x in data]
        sents2 = [x['sentence2'] for x in data]
        lf1 = [x['lf1'] for x in data]
        lf2 = [x['lf2'] for x in data]
        labels = [x['label'] for x in data]
        # lf_length = [len(x['lf1']) for x in data] 
        encoding_sent = self.tokenizer(sents1, sents2, return_tensors='pt', padding=True, truncation=True)
        encoding_lf = self.tokenizer(lf1, lf2, return_tensors='pt', padding=True, truncation=True)
        encoding_lf['input_ids_lf'] = encoding_lf.pop('input_ids')
        encoding_lf['attention_mask_lf'] = encoding_lf.pop('attention_mask')
        encoding_lf['token_type_ids_lf'] = encoding_lf.pop('token_type_ids')
        # token_ids = torch.LongTensor(encoding_sent['input_ids'])
        # lf_ids = torch.LongTensor(encoding_lf['input_ids'])
        # attention_mask_sen = torch.LongTensor(encoding_sent['attention_mask'])
        # attention_mask_lf = torch.LongTensor(encoding_lf['attention_mask'])
        # token_type_ids_sen = torch.LongTensor(encoding_sent['token_type_ids'])
        # token_type_lfs = torch.LongTensor(encoding_lf['token_type_ids'])
        labels = torch.LongTensor(labels)

        return encoding_sent, encoding_lf, labels

    def collate_fn(self, all_data):
        all_data.sort(key=lambda x: -x['token_len'])  # sort by number of tokens

        batches = []
        num_batches = int(np.ceil(len(all_data) / self.batch_size))

        for i in range(num_batches):
            start_idx = i * self.batch_size
            data = all_data[start_idx: start_idx + self.batch_size]

            encoding_sent, encoding_lf, labels = self.pad_data(data)
            batches.append({
                'encoding_sent': encoding_sent,
                'encoding_lf': encoding_lf,
                'labels': labels
            })

        return batches




class BertDataset_no_lf(Dataset):
    # def __init__(self, dataset, tokenizer, lf_tokenizer, batchsize):
    def __init__(self, dataset, tokenizer, batchsize):
        self.dataset = dataset
        self.batch_size = batchsize
        self.tokenizer = tokenizer
        # self.lf_tokenizer = lf_tokenizer
    def __len__(self):
        return len(self.dataset)

    def __getitem__(self, idx):
        ele = self.dataset[idx]
        return ele

    def pad_data(self, data):
        sents1 = [x['sentence1'] for x in data]
        sents2 = [x['sentence2'] for x in data]
        lf1 = ["" for x in data]
        lf2 = ["" for x in data]
        labels = [x['label'] for x in data]
        # lf_length = [len(x['lf1']) for x in data] 
        encoding_sent = self.tokenizer(sents1, sents2, return_tensors='pt', padding=True, truncation=True)
        encoding_lf = self.tokenizer(lf1, lf2, return_tensors='pt', padding=True, truncation=True)
        encoding_lf['input_ids_lf'] = encoding_lf.pop('input_ids')
        encoding_lf['attention_mask_lf'] = encoding_lf.pop('attention_mask')
        encoding_lf['token_type_ids_lf'] = encoding_lf.pop('token_type_ids')
        # token_ids = torch.LongTensor(encoding_sent['input_ids'])
        # lf_ids = torch.LongTensor(encoding_lf['input_ids'])
        # attention_mask_sen = torch.LongTensor(encoding_sent['attention_mask'])
        # attention_mask_lf = torch.LongTensor(encoding_lf['attention_mask'])
        # token_type_ids_sen = torch.LongTensor(encoding_sent['token_type_ids'])
        # token_type_lfs = torch.LongTensor(encoding_lf['token_type_ids'])
        labels = torch.LongTensor(labels)

        return encoding_sent, encoding_lf, labels

    def collate_fn(self, all_data):
        all_data.sort(key=lambda x: -x['token_len'])  # sort by number of tokens

        batches = []
        num_batches = int(np.ceil(len(all_data) / self.batch_size))

        for i in range(num_batches):
            start_idx = i * self.batch_size
            data = all_data[start_idx: start_idx + self.batch_size]

            encoding_sent, encoding_lf, labels = self.pad_data(data)
            batches.append({
                'encoding_sent': encoding_sent,
                'encoding_lf': encoding_lf,
                'labels': labels
            })

        return batches
"""





class BertLFDataset(Dataset):
    def __init__(self, dataset, tokenizer, lf_tokenizer, batchsize):
        self.dataset = dataset
        self.batch_size = batchsize
        self.tokenizer = tokenizer
        self.lf_tokenizer = lf_tokenizer

    def __len__(self):
        return len(self.dataset)

    def __getitem__(self, idx):
        ele = self.dataset[idx]
        return ele

    def pad_data(self, data):
        sents1 = [x['sentence1'] for x in data]
        sents2 = [x['sentence2'] for x in data]
        lf1 = [x['lf1'] for x in data]
        lf2 = [x['lf2'] for x in data]

        texts = [ {
            "sent1": a,
            "sent2": b,
            "lf1": c,
            "lf2": d
        } for a, b, c, d in zip(sents1, sents2, lf1, lf2) ]

        labels = [x['label'] for x in data]
        # lf_length = [len(x['lf1']) for x in data]
        encoding_sent = self.tokenizer(sents1, sents2, return_tensors='pt', padding=True, truncation=True)
        lf_token_idxs, lf_var_idxs, lf_token_var_idx_lens = self.lf_tokenizer.tokenize(lf1 + lf2, expository=False, non_var_idx=0, pad_token_idx=0, pad_var_idx=0)
        lf_token_idxs = torch.LongTensor(lf_token_idxs)
        lf_var_idxs = torch.LongTensor(lf_var_idxs)
        lf_token_var_idx_lens = torch.LongTensor(lf_token_var_idx_lens)
        # token_ids = torch.LongTensor(encoding_sent['input_ids'])
        # lf_ids = torch.LongTensor(encoding_lf['input_ids'])
        # attention_mask_sen = torch.LongTensor(encoding_sent['attention_mask'])
        # attention_mask_lf = torch.LongTensor(encoding_lf['attention_mask'])
        # token_type_ids_sen = torch.LongTensor(encoding_sent['token_type_ids'])
        # token_type_lfs = torch.LongTensor(encoding_lf['token_type_ids'])
        labels = torch.LongTensor(labels)

        return encoding_sent, lf_token_idxs, lf_var_idxs, lf_token_var_idx_lens, labels, texts

    def collate_fn(self, all_data):
        all_data.sort(key=lambda x: -x['token_len'])  # sort by number of tokens

        batches = []
        num_batches = int(np.ceil(len(all_data) / self.batch_size))

        for i in range(num_batches):
            start_idx = i * self.batch_size
            data = all_data[start_idx: start_idx + self.batch_size]

            # encoding_sent, lf_token_idxs, lf_var_idxs, lf_token_var_idx_lens, labels = self.pad_data(data)
            things = self.pad_data(data)
            batches.append({
                'encoding_sent':            things[0],
                'lf_token_idxs':            things[1],
                "lf_var_idxs":              things[2],
                "lf_token_var_idx_lens":    things[3],
                'labels':                   things[4],
                'texts':                    things[5]
            })

        return batches





class BertLFDataset_no_lf(Dataset):
    def __init__(self, dataset, tokenizer, lf_tokenizer, batchsize):
        self.dataset = dataset
        self.batch_size = batchsize
        self.tokenizer = tokenizer
        self.lf_tokenizer = lf_tokenizer

    def __len__(self):
        return len(self.dataset)

    def __getitem__(self, idx):
        ele = self.dataset[idx]
        return ele

    def pad_data(self, data):
        sents1 = [x['sentence1'] for x in data]
        sents2 = [x['sentence2'] for x in data]
        lf1 = ['<null>' for x in data]
        lf2 = ['<null>' for x in data]

        texts = [ {
            "sent1": a,
            "sent2": b,
            "lf1": c,
            "lf2": d
        } for a, b, c, d in zip(sents1, sents2, lf1, lf2) ]

        labels = [x['label'] for x in data]
        # lf_length = [len(x['lf1']) for x in data] 
        encoding_sent = self.tokenizer(sents1, sents2, return_tensors='pt', padding=True, truncation=True)
        lf_token_idxs, lf_var_idxs, lf_token_var_idx_lens = self.lf_tokenizer.tokenize(lf1 + lf2, expository=False, non_var_idx=0, pad_token_idx=0, pad_var_idx=0)
        lf_token_idxs = torch.LongTensor(lf_token_idxs)
        lf_var_idxs = torch.LongTensor(lf_var_idxs)
        lf_token_var_idx_lens = torch.LongTensor(lf_token_var_idx_lens)
        # token_ids = torch.LongTensor(encoding_sent['input_ids'])
        # lf_ids = torch.LongTensor(encoding_lf['input_ids'])
        # attention_mask_sen = torch.LongTensor(encoding_sent['attention_mask'])
        # attention_mask_lf = torch.LongTensor(encoding_lf['attention_mask'])
        # token_type_ids_sen = torch.LongTensor(encoding_sent['token_type_ids'])
        # token_type_lfs = torch.LongTensor(encoding_lf['token_type_ids'])
        labels = torch.LongTensor(labels)

        return encoding_sent, lf_token_idxs, lf_var_idxs, lf_token_var_idx_lens, labels, texts

    def collate_fn(self, all_data):
        all_data.sort(key=lambda x: -x['token_len'])  # sort by number of tokens

        batches = []
        num_batches = int(np.ceil(len(all_data) / self.batch_size))

        for i in range(num_batches):
            start_idx = i * self.batch_size
            data = all_data[start_idx: start_idx + self.batch_size]

            # encoding_sent, lf_token_idxs, lf_var_idxs, lf_token_var_idx_lens, labels = self.pad_data(data)
            things = self.pad_data(data)
            batches.append({
                'encoding_sent':            things[0],
                'lf_token_idxs':            things[1],
                "lf_var_idxs":              things[2],
                "lf_token_var_idx_lens":    things[3],
                'labels':                   things[4],
                'texts':                    things[5]
            })

        return batches





"""
Generate dataset

"""


label_list = ['entailment','neutral','contradiction']
label_to_id = {v: i for i, v in enumerate(label_list)}
tokenizer = AlbertTokenizer.from_pretrained("albert-base-v2")
# lf_tokenizer = XXX
def create_dataset(path):
    result = []
    with open(path) as f:
        data = f.readlines()
    for dp in data:
        dp = json.loads(dp)
        if dp['gold_label'] in label_list:
            dp['label'] = label_to_id[dp['gold_label']]
            dp['token_len'] = len(tokenizer.tokenize(dp['sentence1'])) + len(tokenizer.tokenize(dp['sentence2']))
            result.append(dp)
    return result

train_data = create_dataset("../realdata/finetune_set_lf1-preproc.jsonl")
dev_data_full = create_dataset("../realdata/multinli_1.0_dev_original-preproc.jsonl")
dev_data_half = create_dataset("../realdata/multi_nli_dev_half-preproc.jsonl")
dev_data_lf = create_dataset("../realdata/multinli_dev_lf-preproc.jsonl")
test_data_lf = create_dataset("../myjob/test_lf-preproc.jsonl")
test_data_original = create_dataset("../myjob/test_original-preproc.jsonl")



"""# Model

"""

from transformers import AutoModel,AutoConfig
class Dual_Albert_LogicalModel(nn.Module): #two parallel albert, one for original sentence, one for logical forms
    def __init__(self):
        super().__init__()
        self.num_labels = 3
        self.config = AutoConfig.from_pretrained('albert-base-v2')
        # self.albert = AlbertModel.from_pretrained('albert-base-v2')
        self.albert_sen = AutoModel.from_pretrained('albert-base-v2')
        self.albert_lf = AutoModel.from_pretrained('albert-base-v2')
        self.dropout = nn.Dropout(self.config.classifier_dropout_prob)
        # self.dropout = nn.Dropout(0.1)
        self.embedding_size = self.config.hidden_size * 2
        self.classifier = nn.Linear(self.embedding_size, self.num_labels) 
        # self.init_weights()

    def forward(
        self,
        input_ids=None,
        input_ids_lf=None,
        attention_mask=None,
        attention_mask_lf=None,
        token_type_ids=None,
        token_type_ids_lf=None,
        position_ids=None,
        position_ids_lf=None,
        head_mask=None,
        head_mask_lf=None,
        inputs_embeds=None,
        inputs_embeds_lf=None,
        labels=None,
        output_attentions=None,
        output_attentions_lf=None,
        output_hidden_states=None,
        output_hidden_states_lf=None,
        return_dict=None,   #what's this?
        ):
        return_dict = return_dict if return_dict is not None else self.config.use_return_dict #maybe don't need this?
        #get sentence output from albert_sen
        sentence_outputs = self.albert_sen(
            input_ids=input_ids,
            attention_mask=attention_mask,
            token_type_ids=token_type_ids,
            position_ids=position_ids,
            head_mask=head_mask_lf,
            inputs_embeds=inputs_embeds,
            output_attentions=output_attentions,
            output_hidden_states=output_hidden_states,
            return_dict=return_dict
        )
        pooled_sentence_outputs = sentence_outputs[1]
        pooled_sentence_outputs = self.dropout(pooled_sentence_outputs)


        #get  logic output from albert_lf
        logic_outputs = self.albert_lf(
            input_ids=input_ids_lf,   
            attention_mask=attention_mask_lf,
            token_type_ids=token_type_ids_lf,
            position_ids=position_ids_lf,
            head_mask=head_mask_lf,
            inputs_embeds=inputs_embeds_lf,
            output_attentions=output_attentions_lf,
            output_hidden_states=output_hidden_states_lf,
            return_dict=return_dict
            )
        pooled_logic_outputs = logic_outputs[1]
        pooled_logic_outputs = self.dropout(pooled_logic_outputs)


        #concat two output
        feature = torch.cat((pooled_sentence_outputs, pooled_logic_outputs), dim=1)
        logits = self.classifier(feature).squeeze()

        loss = None
        if labels is not None:
            if self.config.problem_type is None:
                if self.num_labels == 1:
                    self.config.problem_type = "regression"
                elif self.num_labels > 1 and (labels.dtype == torch.long or labels.dtype == torch.int):
                    self.config.problem_type = "single_label_classification"
                else:
                    self.config.problem_type = "multi_label_classification"

            if self.config.problem_type == "regression":
                loss_fct = MSELoss()
                loss = loss_fct(logits.view(-1, self.num_labels), labels)
            elif self.config.problem_type == "single_label_classification":
                loss_fct = CrossEntropyLoss()
                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))
            elif self.config.problem_type == "multi_label_classification":
                loss_fct = BCEWithLogitsLoss()
                loss = loss_fct(logits, labels)
        
        # if not return_dict:
        #     output = (logits,) + outputs[2:]
        #     return ((loss,) + output) if loss is not None else output

        return SequenceClassifierOutput(
            loss=loss,
            logits=logits)



class Dual_Albert_LSTM_LogicalModel(nn.Module): # albert for original sentence, lstm for logical forms with special tokenizer and embeddings
    def __init__(self, lf_tokenizer, token_emb_init, num_lstm_layers):
        HIDDEN_SIZE = 512
        NUM_LAYERS = num_lstm_layers

        super().__init__()
        self.num_labels = 3
        self.albert_config = AutoConfig.from_pretrained('albert-base-v2')
        self.lstm_config = {
            "hidden_size": HIDDEN_SIZE * 2
        }
        self.albert_sen = AutoModel.from_pretrained('albert-base-v2')
        self.lstm_lf = LFModel(lf_tokenizer, token_emb_init, hidden_size=HIDDEN_SIZE, num_layers=NUM_LAYERS)
        self.dropout = nn.Dropout(self.albert_config.classifier_dropout_prob)
        self.embedding_size = self.albert_config.hidden_size + self.lstm_config["hidden_size"]
        # print("self.lstm_config[\"hidden_size\"]", self.lstm_config["hidden_size"])
        # print("self.embedding_size", self.embedding_size)
        self.classifier = nn.Linear(self.embedding_size, self.num_labels) 

    def forward(
        self,
        input_ids=None,
        input_ids_lf=None,
        attention_mask=None,
        attention_mask_lf=None,
        token_type_ids=None,
        token_type_ids_lf=None,
        position_ids=None,
        position_ids_lf=None,
        head_mask=None,
        head_mask_lf=None,
        inputs_embeds=None,
        inputs_embeds_lf=None,
        labels=None,
        output_attentions=None,
        output_attentions_lf=None,
        output_hidden_states=None,
        output_hidden_states_lf=None,
        token_idx_seqs=None,
        var_idx_seqs=None,
        token_var_idx_lens=None,
        non_var_idx=None,
        return_dict=None,
        ):
        return_dict = return_dict if return_dict is not None else self.albert_config.use_return_dict #maybe don't need this?
        #get sentence output from albert_sen
        sentence_outputs = self.albert_sen(
            input_ids=input_ids,
            attention_mask=attention_mask,
            token_type_ids=token_type_ids,
            position_ids=position_ids,
            head_mask=head_mask_lf,
            inputs_embeds=inputs_embeds,
            output_attentions=output_attentions,
            output_hidden_states=output_hidden_states,
            return_dict=return_dict
        )
        pooled_sentence_outputs = sentence_outputs[1]
        pooled_sentence_outputs = self.dropout(pooled_sentence_outputs)


        #get  logic output from LSTM for LF
        #   logic_outputs.shape = (batch size, LFLSTM hidden size)
        logic_outputs = self.lstm_lf(
            token_idx_seqs,
            var_idx_seqs,
            token_var_idx_lens,
            non_var_idx
        )
        # pooled_logic_outputs = logic_outputs[1]
        bsz = logic_outputs.shape[0]
        half_bsz = bsz // 2
        assert half_bsz == bsz - half_bsz, f"bsz is {bsz}..."
        lf1_outputs = logic_outputs[:half_bsz, :]
        lf2_outputs = logic_outputs[half_bsz:, :]
        lf1_outputs = self.dropout(lf1_outputs)
        lf2_outputs = self.dropout(lf2_outputs)


        #concat two output
        # print("pooled_sentence_outputs.shape:", pooled_sentence_outputs.shape)
        # print("lf1_outputs.shape:", lf1_outputs.shape)
        # print("lf2_outputs.shape:", lf2_outputs.shape)
        feature = torch.cat((pooled_sentence_outputs, lf1_outputs, lf2_outputs), dim=1)
        # print("feature.shape:", feature.shape)
        logits = self.classifier(feature).squeeze()

        loss = None
        if labels is not None:
            if self.config.problem_type is None:
                if self.num_labels == 1:
                    self.config.problem_type = "regression"
                elif self.num_labels > 1 and (labels.dtype == torch.long or labels.dtype == torch.int):
                    self.config.problem_type = "single_label_classification"
                else:
                    self.config.problem_type = "multi_label_classification"

            if self.config.problem_type == "regression":
                loss_fct = MSELoss()
                loss = loss_fct(logits.view(-1, self.num_labels), labels)
            elif self.config.problem_type == "single_label_classification":
                loss_fct = CrossEntropyLoss()
                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))
            elif self.config.problem_type == "multi_label_classification":
                loss_fct = BCEWithLogitsLoss()
                loss = loss_fct(logits, labels)
        
        # if not return_dict:
        #     output = (logits,) + outputs[2:]
        #     return ((loss,) + output) if loss is not None else output

        return SequenceClassifierOutput(
            loss=loss,
            logits=logits)



class Siamese_Albert_LogicalModel(AlbertPreTrainedModel):  #siamese albert
    def __init__(self, config):
        super().__init__(config)
        self.num_labels = 3
        self.config = config
        # self.albert = AlbertModel.from_pretrained('albert-base-v2')
        self.albert = AlbertModel(config)
        self.dropout = nn.Dropout(config.classifier_dropout_prob)
        self.embedding_size = config.hidden_size * 2
        self.classifier = nn.Linear(self.embedding_size, self.num_labels,bias=True) 
        self.init_weights()

    def forward(
        self,
        input_ids=None,
        input_ids_lf=None,
        attention_mask=None,
        attention_mask_lf=None,
        token_type_ids=None,
        token_type_ids_lf=None,
        position_ids=None,
        position_ids_lf=None,
        head_mask=None,
        head_mask_lf=None,
        inputs_embeds=None,
        inputs_embeds_lf=None,
        labels=None,
        output_attentions=None,
        output_attentions_lf=None,
        output_hidden_states=None,
        output_hidden_states_lf=None,
        return_dict=None,   #what's this?
        ):
        return_dict = return_dict if return_dict is not None else self.config.use_return_dict
        sentence_outputs = self.albert(
            input_ids=input_ids,
            attention_mask=attention_mask,
            token_type_ids=token_type_ids,
            position_ids=position_ids,
            head_mask=head_mask_lf,
            inputs_embeds=inputs_embeds,
            output_attentions=output_attentions,
            output_hidden_states=output_hidden_states,
            return_dict=return_dict
        )
        pooled_sentence_outputs = sentence_outputs[1]

        pooled_sentence_outputs = self.dropout(pooled_sentence_outputs)

        logic_outputs = self.albert(
            input_ids=input_ids_lf,   
            attention_mask=attention_mask_lf,
            token_type_ids=token_type_ids_lf,
            position_ids=position_ids_lf,
            head_mask=head_mask_lf,
            inputs_embeds=inputs_embeds_lf,
            output_attentions=output_attentions_lf,
            output_hidden_states=output_hidden_states_lf,
            return_dict=return_dict
            )
        pooled_logic_outputs = logic_outputs[1]

        pooled_logic_outputs = self.dropout(pooled_logic_outputs)



        feature = torch.cat((pooled_sentence_outputs, pooled_logic_outputs), dim=1)
        logits = self.classifier(feature).squeeze()

        loss = None
        if labels is not None:
            if self.config.problem_type is None:
                if self.num_labels == 1:
                    self.config.problem_type = "regression"
                elif self.num_labels > 1 and (labels.dtype == torch.long or labels.dtype == torch.int):
                    self.config.problem_type = "single_label_classification"
                else:
                    self.config.problem_type = "multi_label_classification"

            if self.config.problem_type == "regression":
                loss_fct = MSELoss()
                loss = loss_fct(logits.view(-1, self.num_labels), labels)
            elif self.config.problem_type == "single_label_classification":
                loss_fct = CrossEntropyLoss()
                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))
            elif self.config.problem_type == "multi_label_classification":
                loss_fct = BCEWithLogitsLoss()
                loss = loss_fct(logits, labels)
        
        # if not return_dict:
        #     output = (logits,) + outputs[2:]
        #     return ((loss,) + output) if loss is not None else output

        return SequenceClassifierOutput(
            loss=loss,
            logits=logits)


class AlbertForSequenceClassification(AlbertPreTrainedModel):  #original albert solely
    def __init__(self, config):
        super().__init__(config)
        self.num_labels = 3
        self.config = config

        self.albert = AlbertModel(config)
        self.dropout = nn.Dropout(config.classifier_dropout_prob)
        self.classifier = nn.Linear(config.hidden_size, self.config.num_labels)

        self.init_weights()


    def forward(
        self,
        input_ids=None,
        attention_mask=None,
        token_type_ids=None,
        position_ids=None,
        head_mask=None,
        inputs_embeds=None,
        labels=None,
        output_attentions=None,
        output_hidden_states=None,
        return_dict=None,
    ):
        r"""
        labels (:obj:`torch.LongTensor` of shape :obj:`(batch_size,)`, `optional`):
            Labels for computing the sequence classification/regression loss. Indices should be in ``[0, ...,
            config.num_labels - 1]``. If ``config.num_labels == 1`` a regression loss is computed (Mean-Square loss),
            If ``config.num_labels > 1`` a classification loss is computed (Cross-Entropy).
        """
        return_dict = return_dict if return_dict is not None else self.config.use_return_dict

        outputs = self.albert(
            input_ids=input_ids,
            attention_mask=attention_mask,
            token_type_ids=token_type_ids,
            position_ids=position_ids,
            head_mask=head_mask,
            inputs_embeds=inputs_embeds,
            output_attentions=output_attentions,
            output_hidden_states=output_hidden_states,
            return_dict=return_dict,
        )

        pooled_output = outputs[1]

        pooled_output = self.dropout(pooled_output)
        logits = self.classifier(pooled_output)

        loss = None
        if labels is not None:
            if self.config.problem_type is None:
                if self.num_labels == 1:
                    self.config.problem_type = "regression"
                elif self.num_labels > 1 and (labels.dtype == torch.long or labels.dtype == torch.int):
                    self.config.problem_type = "single_label_classification"
                else:
                    self.config.problem_type = "multi_label_classification"

            if self.config.problem_type == "regression":
                loss_fct = MSELoss()
                loss = loss_fct(logits.view(-1, self.num_labels), labels)
            elif self.config.problem_type == "single_label_classification":
                loss_fct = CrossEntropyLoss()
                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))
            elif self.config.problem_type == "multi_label_classification":
                loss_fct = BCEWithLogitsLoss()
                loss = loss_fct(logits, labels)

        if not return_dict:
            output = (logits,) + outputs[2:]
            return ((loss,) + output) if loss is not None else output

        return SequenceClassifierOutput(
            loss=loss,
            logits=logits,
        )

parser = argparse.ArgumentParser(description="train the model")

parser.add_argument(
    "-b", "--batch-size",
    type=int,
    default=12,
    help="batch size"
)

parser.add_argument(
    "-e", "--num-epochs",
    type=int,
    default=4,
    help="number of epochs"
)

parser.add_argument(
    "-lr", "--lr",
    type=float,
    default=1e-6,
    help="learning rate for AdamW"
)

parser.add_argument(
    "-wd", "--wd",
    type=float,
    default=1e-7,
    help="weight decay for AdamW"
)

parser.add_argument(
    "-i", "--token-emb-init",
    type=str,
    default="random",
    help="how do we initialize the token embeddings? random or glove"
)

parser.add_argument(
    "-n", "--num-lstm-layers",
    type=int,
    default=1,
    help="number of LSTM layers"
)

args = parser.parse_args()

batch_size      = args.batch_size
num_epochs      = args.num_epochs
lr              = args.lr
wd              = args.wd
token_emb_init  = args.token_emb_init
num_lstm_layers = args.num_lstm_layers

print(f"Batch size: {batch_size}")
print(f"Number of epochs: {num_epochs}")
print(f"Learning rate: {lr}")
print(f"Weight decay: {wd}")
print(f"Token embedding initialization: {token_emb_init}")
print(f"Number of LSTM layers: {num_lstm_layers}")

"""
Preparing for training
"""

lf_tokenizer = LFTokenizer()
lf_tokenizer.load_syminfo(torch.load("../syminfo.pt"))

bsz = batch_size
train_dataset = BertLFDataset(train_data, tokenizer, lf_tokenizer, bsz)
train_dataloader = DataLoader(train_dataset, shuffle=True, batch_size=bsz,
                                collate_fn=train_dataset.collate_fn)
dev_dataset_full = BertLFDataset(dev_data_full, tokenizer, lf_tokenizer, bsz)
dev_dataloader_full = DataLoader(dev_dataset_full, shuffle=True, batch_size=bsz,
                                  collate_fn=dev_dataset_full.collate_fn)
dev_dataset_half = BertLFDataset(dev_data_half, tokenizer, lf_tokenizer, bsz)
dev_dataloader_half = DataLoader(dev_dataset_half, shuffle=True, batch_size=bsz,
                                  collate_fn=dev_dataset_half.collate_fn)
dev_dataset_lf = BertLFDataset(dev_data_lf, tokenizer, lf_tokenizer, bsz)
dev_dataloader_lf = DataLoader(dev_dataset_lf, shuffle=True, batch_size=bsz,
                                  collate_fn=dev_dataset_lf.collate_fn)
test_dataset_lf = BertLFDataset(test_data_lf, tokenizer, lf_tokenizer, bsz)
test_dataloader_lf = DataLoader(test_dataset_lf, shuffle=True, batch_size=bsz,
                                  collate_fn=test_dataset_lf.collate_fn)
test_dataset_original = BertLFDataset(test_data_original, tokenizer, lf_tokenizer, bsz)
test_dataloader_original = DataLoader(test_dataset_original, shuffle=True, batch_size=bsz,
                                  collate_fn=test_dataset_original.collate_fn)

dev_dataset_full_nolf = BertLFDataset_no_lf(dev_data_full, tokenizer, lf_tokenizer, bsz)
dev_dataloader_full_nolf = DataLoader(dev_dataset_full_nolf, shuffle=True, batch_size=bsz,
                                  collate_fn=dev_dataset_full_nolf.collate_fn)
dev_dataset_half_nolf = BertLFDataset_no_lf(dev_data_half, tokenizer, lf_tokenizer, bsz)
dev_dataloader_half_nolf = DataLoader(dev_dataset_half_nolf, shuffle=True, batch_size=bsz,
                                  collate_fn=dev_dataset_half_nolf.collate_fn)
dev_dataset_lf_nolf = BertLFDataset_no_lf(dev_data_lf, tokenizer, lf_tokenizer, bsz)
dev_dataloader_lf_nolf = DataLoader(dev_dataset_lf_nolf, shuffle=True, batch_size=bsz,
                                  collate_fn=dev_dataset_lf_nolf.collate_fn)
test_dataset_lf_nolf = BertLFDataset_no_lf(test_data_lf, tokenizer, lf_tokenizer, bsz)
test_dataloader_lf_nolf = DataLoader(test_dataset_lf_nolf, shuffle=True, batch_size=bsz,
                                  collate_fn=test_dataset_lf_nolf.collate_fn)
test_dataset_original_nolf = BertLFDataset_no_lf(test_data_original, tokenizer, lf_tokenizer, bsz)
test_dataloader_original_nolf = DataLoader(test_dataset_original_nolf, shuffle=True, batch_size=bsz,
                                  collate_fn=test_dataset_original_nolf.collate_fn)

# for step,batch in enumerate(dev_dataloader_lf_nolf):
#     print(batch[0]['encoding_lf'])
#     break
# for step,batch in enumerate(dev_dataloader_lf):
#     print(batch[0]['encoding_lf'])
#     break

from transformers.optimization import AdamW
# from torch.optim.lr_scheduler import ReduceLROnPlateau
from transformers.modeling_outputs import SequenceClassifierOutput
from transformers.models.albert.modeling_albert import AlbertPreTrainedModel
from transformers.models.albert.configuration_albert import AlbertConfig
from transformers.modeling_utils import PreTrainedModel



"""

Define Model

"""


config = AlbertConfig
# model = AlbertForSequenceClassification.from_pretrained("albert-base-v2",num_labels=3)
# model = Siamese_Albert_LogicalModel.from_pretrained('albert-base-v2') 
# model = Dual_Albert_LogicalModel()

TOKEN_EMB_INIT = token_emb_init

model = Dual_Albert_LSTM_LogicalModel(lf_tokenizer, TOKEN_EMB_INIT, num_lstm_layers)
optimizer_grouped_parameters = [
                {
                    "params": [p for n, p in model.named_parameters()],
                    "weight_decay": 1e-8,   #what decay to use?
                },
            ]
param_size = 0
for d in optimizer_grouped_parameters:
    for p in d['params']:
        tmp_p = p.clone().detach()
        param_size += torch.numel(tmp_p)
print("Parameter size = ", param_size)
optimizer = AdamW(model.parameters(),lr=lr,weight_decay=wd)
# scheduler = ReduceLROnPlateau(optimizer, 'min')
criterion = nn.CrossEntropyLoss()




def model_eval(dataloader, model, print_examples_cnt=0, print_examples_prefix=None):
    # model.eval()

    use_cuda = torch.cuda.is_available()
    running_acc = 0
    total_cnt = 0

    good_examples = []
    bad_examples = []

    for step, batch in enumerate(tqdm(dataloader)):
        assert len(batch) == 1, "batch length not 1"

        encoding_sent, lf_token_idxs, lf_var_idxs, lf_token_var_idx_lens, labels, texts =  batch[0]['encoding_sent'], batch[0]['lf_token_idxs'], batch[0]['lf_var_idxs'], batch[0]['lf_token_var_idx_lens'], batch[0]['labels'], batch[0]['texts']
        total_cnt += labels.size(0)
        if torch.cuda.is_available():
            labels = labels.cuda()
            for k,v in encoding_sent.items():
                encoding_sent[k] = v.cuda()
            lf_token_idxs = lf_token_idxs.cuda()
            lf_var_idxs = lf_var_idxs.cuda()
            lf_token_var_idx_lens = lf_token_var_idx_lens.cuda()

        with torch.no_grad():
            logits = model(**encoding_sent, token_idx_seqs=lf_token_idxs, var_idx_seqs=lf_var_idxs, token_var_idx_lens=lf_token_var_idx_lens, non_var_idx=0).logits
            # logits = model(**encoding_sent).logits   # for original albert
            preds = torch.argmax(logits, dim=1).flatten()
            labels = labels.flatten()
            running_acc += (labels == preds).sum()
            # print(f"batch acc {running_acc / total_cnt}")

        if print_examples_cnt > 0 and (len(good_examples) < print_examples_cnt or len(bad_examples) < print_examples_cnt):
            for k, res in enumerate(labels == preds):
                if res and len(good_examples) < print_examples_cnt:
                    text = texts[k]
                    label = label_list[labels[k]]
                    pred = label_list[preds[k]]
                    good_examples.append((text, label, pred))

                if not(res) and len(bad_examples) < print_examples_cnt:
                    text = texts[k]
                    label = label_list[labels[k]]
                    pred = label_list[preds[k]]
                    bad_examples.append((text, label, pred))

    if print_examples_cnt > 0:
        print(
            "--------------------------------------------------------------------------------\n"
            f"Prefix: {print_examples_prefix}\n\n"
        )

        for texts, label, pred in good_examples:
            sent1 = texts["sent1"]
            sent2 = texts["sent2"]
            lf1   = texts["lf1"]
            lf2   = texts["lf2"]

            print(
                "--------------------------------------------------------------------------------\n"
                "Good example:\n"
                "\n"
                f"Sentence 1:   {sent1}\n"
                f"Sentence 2:   {sent2}\n"
                f"LF 1:         {lf1}\n"
                f"LF 2:         {lf2}\n"
                f"Gold label:   {label}\n"
                f"Prediction:   {pred}\n"
                "\n"
            )

        for texts, label, pred in bad_examples:
            sent1 = texts["sent1"]
            sent2 = texts["sent2"]
            lf1   = texts["lf1"]
            lf2   = texts["lf2"]

            print(
                "--------------------------------------------------------------------------------\n"
                "Bad example:\n"
                "\n"
                f"Sentence 1:   {sent1}\n"
                f"Sentence 2:   {sent2}\n"
                f"LF 1:         {lf1}\n"
                f"LF 2:         {lf2}\n"
                f"Gold label:   {label}\n"
                f"Prediction:   {pred}\n"
                "\n",
                flush=True
            )

    return (running_acc / total_cnt).item()



import sys
import time
from prettytable import PrettyTable, PLAIN_COLUMNS
from tqdm import tqdm, trange
epochs = num_epochs
filepath = "./result_model/model_dualbert_lstm_2e_5_1e_8_epoch4_update/"

if torch.cuda.is_available():
    model.cuda()

for epoch in trange(epochs):
    t1 = time.time()

    print("**********Start Training*************")
    model.train()
    train_loss = 0
    num_batches = 0
    running_acc = 0
    total_cnt = 0

    for step, batch in enumerate(tqdm(train_dataloader)):
        assert len(batch) == 1, "batch length not 1"

        encoding_sent, lf_token_idxs, lf_var_idxs, lf_token_var_idx_lens, labels =  batch[0]['encoding_sent'], batch[0]['lf_token_idxs'], batch[0]['lf_var_idxs'], batch[0]['lf_token_var_idx_lens'], batch[0]['labels']
        if torch.cuda.is_available():
            labels = labels.cuda()
            for k,v in encoding_sent.items():
                encoding_sent[k] = v.cuda()
            lf_token_idxs = lf_token_idxs.cuda()
            lf_var_idxs = lf_var_idxs.cuda()
            lf_token_var_idx_lens = lf_token_var_idx_lens.cuda()

        optimizer.zero_grad()
        logits = model(**encoding_sent, token_idx_seqs=lf_token_idxs, var_idx_seqs=lf_var_idxs, token_var_idx_lens=lf_token_var_idx_lens, non_var_idx=0).logits
        # logits = model(**encoding_sent).logits
        preds = torch.argmax(logits, dim=1).flatten()
        loss = criterion(logits, labels.view(-1))
        # dev_acc = model_eval(batch, model)
        # print("batch loss", loss.item())

        loss.backward()
        # scheduler.step(loss)
        optimizer.step()

        train_loss += loss.item()
        num_batches += 1
        running_acc += (labels == preds).sum().item()
        total_cnt += labels.size(0)

        # if step % 10 == 0:
            # print("running loss", (train_loss / (num_batches)))

    train_loss = train_loss / (num_batches)
    train_acc = running_acc / total_cnt

    print("**********Start Evaluation*************")
    model.eval()

    # train_acc = model_eval(train_dataloader, model)
    dev_acc_full            = model_eval(dev_dataloader_full,           model)
    dev_acc_full_nolf       = model_eval(dev_dataloader_full_nolf,      model)
    dev_acc_half            = model_eval(dev_dataloader_half,           model)
    dev_acc_half_nolf       = model_eval(dev_dataloader_half_nolf,      model)
    dev_acc_lf              = model_eval(dev_dataloader_lf,             model)
    dev_acc_lf_nolf         = model_eval(dev_dataloader_lf_nolf,        model)
    test_acc_lf             = model_eval(test_dataloader_lf,            model, print_examples_cnt=10, print_examples_prefix="test LF")
    test_acc_lf_nolf        = model_eval(test_dataloader_lf_nolf,       model, print_examples_cnt=10, print_examples_prefix="test LF no LF")
    test_acc_original       = model_eval(test_dataloader_original,      model, print_examples_cnt=10, print_examples_prefix="test original")
    test_acc_original_nolf  = model_eval(test_dataloader_original_nolf, model, print_examples_cnt=10, print_examples_prefix="test original no LF")

    # if dev_acc > best_dev_acc:
    #     best_dev_acc = dev_acc
    #     best_model = model
    #     torch.save(best_model, filepath)

    print(
        "================================================================================\n"
        f"Epoch {epoch}\n"
        "\n"
    )

    x = PrettyTable()
    x.header = False
    x.border = False
    x.field_names = ["Thing", "Value"]

    x.add_row([
        "Training loss",
        round(train_loss, 3)
    ])
    x.add_row([
        "Training acc",
        round(train_acc, 3)
    ])

    x.add_row([
        "Dev acc: full dev",
        round(dev_acc_full, 3)
    ])
    x.add_row([
        "Dev acc: full dev no LF",
        round(dev_acc_full_nolf, 3)
    ])

    x.add_row([
        "Dev acc: half LF dev",
        round(dev_acc_half, 3)
    ])
    x.add_row([
        "Dev acc: half LF dev no LF",
        round(dev_acc_half_nolf, 3)
    ])

    x.add_row([
        "Dev acc: LF dev",
        round(dev_acc_lf, 3)
    ])
    x.add_row([
        "Dev acc: LF dev no LF",
        round(dev_acc_lf_nolf, 3)
    ])

    x.add_row([
        "Test acc: LF",
        round(test_acc_lf, 3)
    ])
    x.add_row([
        "Test acc: LF no LF",
        round(test_acc_lf_nolf, 3)
    ])

    x.add_row([
        "Test acc: original",
        round(test_acc_original, 3)
    ])
    x.add_row([
        "Test acc: original no LF",
        round(test_acc_original_nolf, 3)
    ])

    x.align = "l"
    print(x, flush=True)

    t2 = time.time()
    print(f"That took {round(t2 - t1, 3)} seconds.", file=sys.stderr, flush=True)